{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import ftfy\n",
    "import random\n",
    "import pickle\n",
    "import os, os.path\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vocaburary builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)\n",
    "\n",
    "def normalize(word):\n",
    "    return re.sub(r\"\\d\", \"0\", word).lower()\n",
    "\n",
    "\n",
    "def strong_normalize(word):\n",
    "    w = ftfy.fix_text(word.lower())\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", w)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"([^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"([^\\d][^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"``\", '\"', w)\n",
    "    w = re.sub(r\"''\", '\"', w)\n",
    "    w = re.sub(r\"\\d\", \"0\", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def buildVocab(graphs, cutoff=1):\n",
    "    wordsCount = Counter()\n",
    "    charsCount = Counter()\n",
    "    uposCount = Counter()\n",
    "    xposCount = Counter()\n",
    "    relCount = Counter()\n",
    "    featCount = Counter()\n",
    "    langCount = Counter()\n",
    "\n",
    "    for graph in graphs:\n",
    "        wordsCount.update([node.norm for node in graph.nodes[1:]])\n",
    "        for node in graph.nodes[1:]:\n",
    "            charsCount.update(list(node.word))\n",
    "            featCount.update(node.feats_set)\n",
    "            #  charsCount.update(list(node.norm))\n",
    "        uposCount.update([node.upos for node in graph.nodes[1:]])\n",
    "        xposCount.update([node.xupos for node in graph.nodes[1:]])\n",
    "        relCount.update([rel for rel in graph.rels[1:]])\n",
    "        langCount.update([node.lang for node in graph.nodes[1:]])\n",
    "        \n",
    "\n",
    "    wordsCount = Counter({w: i for w, i in wordsCount.items() if i >= cutoff})\n",
    "    print(\"Vocab containing {} words\".format(len(wordsCount)))\n",
    "    print(\"Charset containing {} chars\".format(len(charsCount)))\n",
    "    print(\"UPOS containing {} tags\".format(len(uposCount)), uposCount)\n",
    "    #print(\"XPOS containing {} tags\".format(len(xposCount)), xposCount)\n",
    "    print(\"Rels containing {} tags\".format(len(relCount)), relCount)\n",
    "    print(\"Feats containing {} tags\".format(len(featCount)))\n",
    "    print(\"lang containing {} tags\".format(len(langCount)), langCount)\n",
    "\n",
    "    ret = {\n",
    "        \"vocab\": list(wordsCount.keys()),\n",
    "        \"wordfreq\": wordsCount,\n",
    "        \"charset\": list(charsCount.keys()),\n",
    "        \"charfreq\": charsCount,\n",
    "        \"upos\": list(uposCount.keys()),\n",
    "        \"xpos\": list(xposCount.keys()),\n",
    "        \"rels\": list(relCount.keys()),\n",
    "        \"feats\": list(featCount.keys()),\n",
    "        \"lang\": list(langCount.keys()),\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def shuffled_stream(data):\n",
    "    len_data = len(data)\n",
    "    while True:\n",
    "        for d in random.sample(data, len_data):\n",
    "            yield d\n",
    "\n",
    "def shuffled_balanced_stream(data):\n",
    "    for ds in zip(*[shuffled_stream(s) for s in data]):\n",
    "        ds = list(ds)\n",
    "        random.shuffle(ds)\n",
    "        for d in ds:\n",
    "            yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Classes about data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_dict(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return {}\n",
    "\n",
    "    ret = {}\n",
    "    lst = features.split(\"|\")\n",
    "    for l in lst:\n",
    "        k, v = l.split(\"=\")\n",
    "        ret[k] = v\n",
    "    return ret\n",
    "\n",
    "\n",
    "def parse_features(features):\n",
    "    if features is None or features == \"_\":\n",
    "        return set()\n",
    "\n",
    "    return features.lower().split(\"|\")\n",
    "\n",
    "\n",
    "class Word:\n",
    "\n",
    "    def __init__(self, word, upos, lemma=None, xpos=None, feats=None, misc=None, lang=None):\n",
    "        self.word = word\n",
    "        self.norm = normalize(word) #strong_normalize(word)\n",
    "        self.lemma = lemma if lemma else \"_\"\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos if xpos else \"_\"\n",
    "        self.xupos = self.upos + \"|\" + self.xpos\n",
    "        self.feats = feats if feats else \"_\"\n",
    "        self.feats_set = parse_features(self.feats)\n",
    "        self.misc = misc if misc else \"_\"\n",
    "        self.lang = lang if lang else \"_\"\n",
    "\n",
    "    def cleaned(self):\n",
    "        return Word(self.word, \"_\")\n",
    "\n",
    "    def clone(self):\n",
    "        return Word(self.word, self.upos, self.lemma, self.xpos, self.feats, self.misc)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}_{}\".format(self.word, self.upos)\n",
    "\n",
    "\n",
    "class DependencyGraph(object):\n",
    "\n",
    "    def __init__(self, words, tokens=None):\n",
    "        #  Token is a tuple (start, end, form)\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        self.nodes = np.array([Word(\"*root*\", \"*root*\")] + list(words))\n",
    "        self.tokens = tokens\n",
    "        self.heads = np.array([-1] * len(self.nodes))\n",
    "        self.rels = np.array([\"_\"] * len(self.nodes), dtype=object)\n",
    "\n",
    "    def __copy__(self):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        result.nodes = self.nodes\n",
    "        result.tokens = self.tokens\n",
    "        result.heads = self.heads.copy()\n",
    "        result.rels = self.rels.copy()\n",
    "        return result\n",
    "\n",
    "    def cleaned(self, node_level=True):\n",
    "        if node_level:\n",
    "            return DependencyGraph([node.cleaned() for node in self.nodes[1:]], self.tokens)\n",
    "        else:\n",
    "            return DependencyGraph([node.clone() for node in self.nodes[1:]], self.tokens)\n",
    "\n",
    "    def attach(self, head, tail, rel):\n",
    "        self.heads[tail] = head\n",
    "        self.rels[tail] = rel\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([\"{} ->({})  {} ({})\".format(str(self.nodes[i]), self.rels[i], self.heads[i], self.nodes[self.heads[i]]) for i in range(len(self.nodes))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. IO and CoNLL file reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take it from https://github.com/chantera/biaffineparser/blob/master/utils.py\n",
    "def read_conll(filename, lang_code=None):\n",
    "    \n",
    "    print(\"read_conll with\", lang_code)\n",
    "    def get_word(columns):\n",
    "        return Word(columns[FORM], columns[UPOS], lemma=columns[LEMMA], xpos=columns[XPOS], feats=columns[FEATS], misc=columns[MISC], lang=lang_code)\n",
    "\n",
    "    def get_graph(graphs, words, tokens, edges):\n",
    "        graph = DependencyGraph(words, tokens)\n",
    "        for (h, d, r) in edges:\n",
    "            graph.attach(h, d, r)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    file = open(filename, \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "    graphs = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    edges = []\n",
    "\n",
    "    sentence_start = False\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            if len(words) > 0:\n",
    "                get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "            break\n",
    "        line = line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        # Handle sentence start boundaries\n",
    "        if not sentence_start:\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            # Start a new sentence\n",
    "            sentence_start = True\n",
    "        if not line:\n",
    "            sentence_start = False\n",
    "            if len(words) > 0:\n",
    "                get_graph(graphs, words, tokens, edges)\n",
    "                words, tokens, edges = [], [], []\n",
    "            continue\n",
    "\n",
    "        # Read next token/word\n",
    "        columns = line.split(\"\\t\")\n",
    "\n",
    "        # Skip empty nodes\n",
    "        if \".\" in columns[ID]:\n",
    "            continue\n",
    "\n",
    "        # Handle multi-word tokens to save word(s)\n",
    "        if \"-\" in columns[ID]:\n",
    "            start, end = map(int, columns[ID].split(\"-\"))\n",
    "            tokens.append((start, end + 1, columns[FORM]))\n",
    "\n",
    "            for _ in range(start, end + 1):\n",
    "                word_line = file.readline().rstrip(\"\\r\\n\")\n",
    "                word_columns = word_line.split(\"\\t\")\n",
    "                words.append(get_word(word_columns))\n",
    "                if word_columns[HEAD].isdigit():\n",
    "                    head = int(word_columns[HEAD])\n",
    "                else:\n",
    "                    head = -1\n",
    "                edges.append((head, int(word_columns[ID]), word_columns[DEPREL].split(\":\")[0]))\n",
    "        # Basic tokens/words\n",
    "        else:\n",
    "            words.append(get_word(columns))\n",
    "            if columns[HEAD].isdigit():\n",
    "                head = int(columns[HEAD])\n",
    "            else:\n",
    "                head = -1\n",
    "            edges.append((head, int(columns[ID]), columns[DEPREL].split(\":\")[0]))\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def write_conll(filename, graphs, append=False):\n",
    "    if append:\n",
    "        file = open(filename, \"a\", encoding=\"UTF-8\")\n",
    "    else:\n",
    "        file = open(filename, \"w\", encoding=\"UTF-8\")\n",
    "\n",
    "    for j in range(len(graphs)):\n",
    "        graph = graphs[j]\n",
    "        curtoken = 0\n",
    "        for i in range(1, len(graph.nodes)):\n",
    "            if curtoken < len(graph.tokens) and i == graph.tokens[curtoken][0]:\n",
    "                file.write(\"{}-{}\\t{}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\".format(graph.tokens[curtoken][0], graph.tokens[curtoken][1] - 1, graph.tokens[curtoken][2]))\n",
    "                curtoken += 1\n",
    "\n",
    "            file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t_\\t{}\\n\".format(\n",
    "                i, graph.nodes[i].word, graph.nodes[i].lemma, graph.nodes[i].upos, graph.nodes[i].xpos,\n",
    "                graph.nodes[i].feats, graph.heads[i], graph.rels[i], graph.nodes[i].misc))\n",
    "\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def read_text(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    documents = text.split(\"\\n\\n\")\n",
    "    ret = [\" \".join(x.split(\"\\n\")).strip() for x in documents]\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build Pytorch Model Class\n",
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "    \n",
    "def _model_var(model, x):\n",
    "    p = next(model.parameters())\n",
    "    if p.is_cuda:\n",
    "        x = x.cuda(p.get_device())\n",
    "    return torch.autograd.Variable(x)\n",
    "\n",
    "def pad_sequence(xs, length=None, padding=0, dtype=np.int64):\n",
    "    lengths = [len(x) for x in xs]\n",
    "    if length is None:\n",
    "        length = max(lengths)\n",
    "    y = np.array([np.pad(x.astype(dtype), (0, length - l),\n",
    "                         mode=\"constant\", constant_values=padding)\n",
    "                  for x, l in zip(xs, lengths)])\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take it from https://github.com/chantera/teras/blob/master/teras/framework/pytorch/model.py\n",
    "class MLP(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        assert all(type(layer) == MLP.Layer for layer in layers)\n",
    "        super(MLP, self).__init__(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    class Layer(nn.Linear):\n",
    "\n",
    "        def __init__(self, in_features, out_features,\n",
    "                     activation=None, dropout=0.0, bias=True):\n",
    "            super(MLP.Layer, self).__init__(in_features, out_features, bias)\n",
    "            if activation is None:\n",
    "                self._activate = lambda x: x\n",
    "            else:\n",
    "                if not callable(activation):\n",
    "                    raise ValueError(\"activation must be callable: type={}\"\n",
    "                                     .format(type(activation)))\n",
    "                self._activate = activation\n",
    "            assert dropout == 0 or type(dropout) == float\n",
    "            self._dropout_ratio = dropout\n",
    "            if dropout > 0:\n",
    "                self._dropout = nn.Dropout(p=self._dropout_ratio)\n",
    "            else:\n",
    "                self._dropout = lambda x: x\n",
    "\n",
    "        def forward(self, x):\n",
    "            size = x.size()\n",
    "            if len(size) > 2:\n",
    "                y = super(MLP.Layer, self).forward(\n",
    "                    x.contiguous().view(-1, size[-1]))\n",
    "                y = y.view(size[0:-1] + (-1,))\n",
    "            else:\n",
    "                y = super(MLP.Layer, self).forward(x)\n",
    "            return self._dropout(self._activate(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, meta_hidden_size, meta_layer_size,\n",
    "                 meta_directions, meta_mlp_hidden, num_class, dropout_ratio):\n",
    "        super(MetaLSTM, self).__init__()\n",
    "        \n",
    "        self.meta_hidden_size = 300\n",
    "        self.meta_layer_size = 1\n",
    "        self.meta_directions = 2\n",
    "        self.meta_mlp_hidden = 300\n",
    "        self.meta_input_dim = input_dim\n",
    "        \n",
    "        self.meta_LSTM = nn.LSTM(self.meta_input_dim, self.meta_hidden_size, self.meta_layer_size, dropout=dropout_ratio, batch_first=False, bidirectional=True)        \n",
    "        layers = [MLP.Layer(self.meta_hidden_size * self.meta_directions, self.meta_mlp_hidden, F.elu, dropout_ratio) for i in range(1)]\n",
    "        self.meta_MLP = MLP(layers)\n",
    "        self.meta_linear = nn.Linear(self.meta_mlp_hidden, num_class, bias=True)\n",
    "        \n",
    "    def forward(self, input_vector, batch_lengths):\n",
    "        # input_vec = b,s,d\n",
    "        \n",
    "        input_vecs = input_vector\n",
    "        lengths = batch_lengths\n",
    "        \n",
    "        indices = np.argsort(-np.array(lengths)).astype(np.int64) #sorting based on seq_len\n",
    "        lengths = lengths[indices]\n",
    "        input_vecs = torch.stack([input_vecs[idx] for idx in indices]) #In order to put batched-LSTM: ordering inputs and stacking input_vecs = 3 x 30 x 9        \n",
    "        input_vecs = nn.utils.rnn.pack_padded_sequence(input_vecs, lengths, batch_first=True) #padded -> pack to eliminate processes for paddings  input_vecs = lengths*batch x feature_dim = 58 x 9\n",
    "        encode_out = self.meta_LSTM(input_vecs)[0] #encode_out = batch x lengths*batch x hidden_size*2 =  58x16\n",
    "        encode_out = nn.utils.rnn.pad_packed_sequence(encode_out, batch_first=True)[0] #packed -> pad #3x30x16\n",
    "        encode_out = encode_out.index_select(dim=0, index=_model_var(self, torch.from_numpy(np.argsort(indices).astype(np.int64))))\n",
    "\n",
    "        #MLP-based shape resizing\n",
    "        mlp_logits = self.meta_MLP(encode_out)\n",
    "        meta_linear = self.meta_linear(mlp_logits)\n",
    "        pred = meta_linear.data.max(2)[1].cpu()\n",
    "        \n",
    "        return mlp_logits, meta_linear, pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encode_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_word, \n",
    "                 dim_word,\n",
    "                 num_char,\n",
    "                 dim_char,\n",
    "                 num_pos, \n",
    "                 dim_pos, \n",
    "                 num_rel, \n",
    "                 dim_rel,\n",
    "                 num_lang,\n",
    "                 dim_lang,\n",
    "                 ext_word_emb,\n",
    "                 ext_word_size,\n",
    "                 dim_ext_word,\n",
    "                 enc_hidden_size, \n",
    "                 enc_layer_size, \n",
    "                 mlp_hidden_size,\n",
    "                 char_active,\n",
    "                 char_global_active,\n",
    "                 elmo_active,\n",
    "                 postagger_active,\n",
    "                 elmo_weight_file,\n",
    "                 elmo_option_file,\n",
    "                 cuda_device\n",
    "                ):\n",
    "        super(Encode_model, self).__init__()\n",
    "        \n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "        #Step0: init variables\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.enc_layer_size = enc_layer_size\n",
    "        self.num_directions = 2 #static\n",
    "        self.arc_mlp_hidden = mlp_hidden_size\n",
    "        self.dep_mlp_hidden = 100 #static\n",
    "        self.dropout_ratio = 0.33 #static\n",
    "        \n",
    "        self.ext_word_dim = 0 #static\n",
    "        \n",
    "        self.char_active = char_active\n",
    "        self.char_global_active = char_global_active\n",
    "        self.char_cnn_active = False# \n",
    "\n",
    "        self.char_hidden_size = 300 #static ####KKL\n",
    "        self.char_layer_size = 3 #static\n",
    "        self.char_directions = 2 #static\n",
    "        self.char_mlp_size = 1 #static\n",
    "        self.char_global_mlp_size = 1 #static\n",
    "        self.char_feature_dim = 0 #static\n",
    "        self.char_global_feature_dim = 0\n",
    "        \n",
    "        self.num_pos = num_pos\n",
    "        self.pos_hidden_size = enc_hidden_size\n",
    "        self.pos_layer_size = enc_layer_size\n",
    "        self.pos_directions = 2 #static\n",
    "        self.pos_mlp_hidden = 300\n",
    "        \n",
    "        self.dim_lang = dim_lang\n",
    "        self.num_lang = num_lang\n",
    "        \n",
    "        self.postagger_active = postagger_active\n",
    "        self.elmo = None #static\n",
    "        self.elmo_active = elmo_active\n",
    "        self.elmo_weight_file = elmo_weight_file\n",
    "        self.elmo_option_file = elmo_option_file\n",
    "        self.elmo_dim = 0 #static\n",
    "        self.elmo_hidden_size = 0\n",
    "        \n",
    "\n",
    "        #Step1: Preparing external word embeddings\n",
    "        if ext_word_emb is not None:\n",
    "            num_ext_emb, dim_ext_emb = ext_word_emb.shape\n",
    "            self.ext_emb = nn.Embedding(*ext_word_emb.shape, padding_idx=0)\n",
    "            self.ext_emb.cpu() # load embeddings on cpu\n",
    "            self.ext_emb.weight = nn.Parameter(torch.from_numpy(ext_word_emb))\n",
    "            #self.ext_emb.weight.data._copy(torch.from_numpy(ext_word_emb))\n",
    "            self.ext_emb.weight.requires_grad = False\n",
    "            self.ext_word_dim = dim_ext_emb\n",
    "            print(\"The dimension of pre-trained word embedding: \", self.ext_word_dim)\n",
    "        elif ext_word_size > 0 and dim_ext_word > 0:\n",
    "            self.ext_emb = nn.Embedding(ext_word_size + 3, dim_ext_word, padding_idx=0)\n",
    "            self.ext_emb.cpu()\n",
    "            self.ext_emb.weight.requires_grad = False\n",
    "            self.ext_word_dim = dim_ext_word\n",
    "        else:\n",
    "            print(\"Init without external embeding\")\n",
    "\n",
    "                \n",
    "        #Step2: Preparing ELMo embeddings\n",
    "        if self.elmo_active:\n",
    "            from allennlp.modules.elmo import Elmo\n",
    "            self.elmo_dim = 1024\n",
    "            if torch.cuda.is_available() and self.cuda_device != -1: self.elmo = Elmo(options_file=self.elmo_option_file , weight_file=self.elmo_weight_file, num_output_representations=2).cuda(self.cuda_device)\n",
    "            else: self.elmo = Elmo(options_file=self.elmo_option_file , weight_file=self.elmo_weight_file, num_output_representations=2)\n",
    "\n",
    "            # Elmo,\n",
    "            from allennlp.modules.scalar_mix import ScalarMix\n",
    "            self.elmo_hidden_size = 1024 \n",
    "            self.scalar_mix_x_in = ScalarMix(2, False) # scalar vec dim, layer_normalization\n",
    "            #self.x_elmo_layer_in = nn.Linear(self.elmo_dim, self.elmo_hidden_size)\t# dimension reduction\n",
    "\n",
    "        #Step3: Initializing char embeddings\n",
    "        if self.char_active: ## With Structured-self attentive embedding \n",
    "            self.char_feature_dim = 100   ####KKL\n",
    "            self.char_emb = nn.Embedding(num_char + 3, dim_char, padding_idx=0)\n",
    "            self.char_LSTM = nn.LSTM(dim_char, self.char_hidden_size, self.char_layer_size, dropout=self.dropout_ratio, bidirectional=True)\n",
    "            layers = [MLP.Layer(self.char_hidden_size * self.char_directions, self.char_mlp_size, F.elu, 0) for i in range(1)]\n",
    "            self.char_mlp = MLP(layers)\n",
    "            self.att_score = nn.Softmax(dim=2)\n",
    "            self.char_linear = nn.Linear(self.char_mlp_size * self.char_hidden_size * self.char_directions, self.char_feature_dim, bias=False)\n",
    "        elif self.char_cnn_active: ## With CNN based embedding\n",
    "            self.char_feature_dim = 100\n",
    "            self.embedding4char = nn.Embedding(num_char + 3, dim_char, padding_idx=0)\n",
    "            self.char_cnn = CNN_Text(\n",
    "                    input_size=int(self.char_feature_dim/2),\n",
    "                    output_size=int(self.char_feature_dim/2),\n",
    "                    filter_sizes=[2,3,4,5],\n",
    "                    num_filters=30,\n",
    "                    dropout_rate=self.dropout_ratio,\n",
    "            )\n",
    "            \n",
    "            #Step3-1: Initializing char global embeddings    \n",
    "        if self.char_global_active:\n",
    "            self.char_global_feature_dim = 100   ####KKL\n",
    "            self.char_global_emb = nn.Embedding(num_char + 3, dim_char, padding_idx=0)\n",
    "            self.char_global_LSTM = nn.LSTM(dim_char, self.char_hidden_size, self.char_layer_size, dropout=self.dropout_ratio, bidirectional=True)\n",
    "            layers = [MLP.Layer(self.char_hidden_size * self.char_directions, self.char_global_mlp_size, F.elu, 0) for i in range(1)]\n",
    "            self.char_global_mlp = MLP(layers)\n",
    "            self.att_global_score = nn.Softmax()\n",
    "            self.char_global_linear = nn.Linear(self.char_global_mlp_size * self.char_hidden_size * self.char_directions, num_pos, bias=True)\n",
    "        \n",
    "        #Step4: Initialzing Encoder\n",
    "            #4-0. init embeddings\n",
    "        self.word_emb = nn.Embedding(num_word + 3, dim_word, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_ratio)\n",
    "        self.pos_emb = nn.Embedding(num_pos, dim_pos, padding_idx=0)\n",
    "        \n",
    "        if self.dim_lang > 0:\n",
    "            self.lang_emb = nn.Embedding(self.num_lang + 2, self.dim_lang, padding_idx=0)\n",
    "\n",
    "            #4-1. init Pos Encoder and bilinear Classifier\n",
    "        if self.postagger_active:\n",
    "            self.pos_input_dim = self.char_hidden_size * self.char_directions + self.ext_word_dim + self.dim_lang \n",
    "            self.pos_LSTM = nn.LSTM(self.pos_input_dim, self.pos_hidden_size, self.pos_layer_size, dropout=self.dropout_ratio, batch_first=False, bidirectional=True)        \n",
    "            layers = [MLP.Layer(self.pos_hidden_size * self.pos_directions, self.pos_mlp_hidden, F.elu, self.dropout_ratio) for i in range(1)]\n",
    "            self.pos_MLP = MLP(layers)\n",
    "            self.pos_linear = nn.Linear(self.pos_mlp_hidden, num_pos, bias=True)\n",
    "\n",
    "            #4-2. init Encoder\n",
    "        self.input_dim = dim_word + self.ext_word_dim + self.char_feature_dim + self.elmo_hidden_size + self.dim_lang \n",
    "        self.enc_LSTM = nn.LSTM(self.input_dim, enc_hidden_size, enc_layer_size, dropout=self.dropout_ratio, batch_first=False, bidirectional=True)        \n",
    "        \n",
    "            #4-3. init Decoder with MLPs for arc and dep\n",
    "        if False: layers_input = self.enc_hidden_size * self.num_directions + self.elmo_hidden_size\n",
    "        else: layers_input = self.enc_hidden_size * self.num_directions\n",
    "        layers = [MLP.Layer(layers_input, self.arc_mlp_hidden, F.elu, self.dropout_ratio) for i in range(1)]\n",
    "        self.arc_MLP_rel = MLP(layers)\n",
    "        self.pos2_linear = nn.Linear(self.arc_mlp_hidden, num_pos, bias=True)\n",
    "        \n",
    "        self.meta_input_dim = self.arc_mlp_hidden + dim_pos + self.dim_lang #+ self.char_hidden_size * self.char_directions\n",
    "        self.metaLSTM = MetaLSTM(self.meta_input_dim,300,1,2,300,num_pos, self.dropout_ratio)\n",
    "        \n",
    "\n",
    "    def forward(self, word_seqs, pos_seqs, ext_word_seqs, char_seqs, token_seqs, lang_seqs, seq_lenths=None, train=True):\n",
    "        \n",
    "        # #print(token_seqs)\n",
    "        #Set batch and lenth for padding\n",
    "        batch_size = len(word_seqs)\n",
    "        lengths = seq_lenths\n",
    "        \n",
    "        #Getting word embeddings\n",
    "        word_seqs = self.cuda_variable(torch.from_numpy(word_seqs)) # word_seq = batch x seq_IDs\n",
    "        words_vecs = self.dropout(self.word_emb(word_seqs)) #words_vecs = batch x len(seq) x hidden_size = 3 x 30 x 5\n",
    "        \n",
    "        #Processing ELMo embeddings\n",
    "        if self.elmo_active:\n",
    "            import make_elmo_cids\n",
    "            elmo_cids = make_elmo_cids.batch_to_ids(token_seqs)\n",
    "\n",
    "            if torch.cuda.is_available() and self.cuda_device != -1:\n",
    "                elmo_cids = elmo_cids.cuda()\n",
    "\n",
    "            elmo_emb = self.elmo(elmo_cids)['elmo_representations']\n",
    "                \n",
    "            \n",
    "            x_ELMo_in = self.scalar_mix_x_in(torch.stack([elmo_emb[0], elmo_emb[1]]))\n",
    "            x_ELMo_in = nn.functional.dropout(x_ELMo_in, p=0.5, training=self.training)\n",
    "\n",
    "            if not self.char_active and not self.char_cnn_active:\n",
    "                words_vecs = torch.cat((words_vecs, x_ELMo_in), -1)\n",
    "\n",
    "        #Processing self attention representation for char-level word embedding\n",
    "        if self.char_active:\n",
    "            \"\"\"\n",
    "            char_seqs: list(list(sentence))\n",
    "             - B: batch_size = len(char_seqs)\n",
    "            \"\"\"\n",
    "            batch_size, word_lenth, word_dim = words_vecs.size()\n",
    "            words_chars_vecs = self.cuda_variable(torch.zeros(batch_size, word_lenth, word_dim + self.char_feature_dim + self.elmo_hidden_size))            \n",
    "            for sent_idx, sentence in enumerate(char_seqs):\n",
    "                \"\"\"\n",
    "                 - sentence: list(tokens)\n",
    "                 - B: batch_size = len(sentence) = size of tokens\n",
    "                 - S: seq_len = len(token) = size of chars\n",
    "                \"\"\"\n",
    "                char_batch_size = len(sentence) #number of tokens == batch_size\n",
    "                char_len_list = np.array([len(token) for token in sentence])\n",
    "\n",
    "                h0 = self.cuda_variable(torch.zeros(self.char_layer_size*self.char_directions, char_batch_size, self.char_hidden_size))\n",
    "                c0 = self.cuda_variable(torch.zeros(self.char_layer_size*self.char_directions, char_batch_size, self.char_hidden_size))\n",
    "                \n",
    "                pad_sentence = pad_sequence(sentence)\n",
    "                char_local_seqs = self.cuda_variable(torch.from_numpy(pad_sentence))\n",
    "                char_vecs = self.char_emb(char_local_seqs)\n",
    "\n",
    "                indices = np.argsort(-np.array(char_len_list)).astype(np.int64) #Searching \"char_len_list\"'s dec order\n",
    "                char_len_list_ordered = char_len_list[indices] #Sorting \"char_len_list\" based on the indices\n",
    "                char_vecs_ordered = torch.stack([char_vecs[order] for order in indices]) #Stacking batches as an embedding based on dec order                \n",
    "                char_sentence_packed = nn.utils.rnn.pack_padded_sequence(char_vecs_ordered, char_len_list_ordered, batch_first=True)\n",
    "                char_lstm_batch_out = self.char_LSTM(char_sentence_packed)[0] # (Batch * seq_len x  2*LSTM_hidden)\n",
    "                char_lstm_batch_out = nn.utils.rnn.pad_packed_sequence(char_lstm_batch_out, batch_first=True)[0] # (Batch x seq_len x 2*LSTM_hidden)\n",
    "                char_lstm_batch_out = char_lstm_batch_out.index_select(dim=0, index=_model_var(self, torch.from_numpy(np.argsort(indices).astype(np.int64)))) #make it back unorder\n",
    "                \n",
    "                #print(char_lstm_batch_out)\n",
    "                \n",
    "                char_decode_out = self.char_mlp(char_lstm_batch_out)\n",
    "                \n",
    "                #print(char_decode_out)\n",
    "                char_att_score = self.att_score(char_decode_out.transpose(1, 2))\n",
    "                char_att_features = char_att_score.bmm(char_lstm_batch_out)\n",
    "                char_att_features = char_att_features.view(char_att_features.size(0), -1) # B x seq_len * 2 * mlp_hidden\n",
    "                char_features = self.char_linear(char_att_features)\n",
    "                \n",
    "                if self.elmo_active:\n",
    "                    words_chars_vecs[sent_idx, :char_features.size(0)] = torch.cat((words_vecs[sent_idx, :char_features.size(0)], x_ELMo_in[sent_idx, :char_features.size(0)], char_features), -1)\n",
    "                else:\n",
    "                    words_chars_vecs[sent_idx, :char_features.size(0)] = torch.cat((words_vecs[sent_idx, :char_features.size(0)], char_features), -1)\n",
    "            words_vecs = words_chars_vecs  \n",
    "            \n",
    "        \n",
    "        elif self.char_cnn_active:\n",
    "            import pdb; pdb.set_trace()\n",
    "            xc1_emb = nn.functional.dropout(char_vecs, p=0.2, training=self.training)\n",
    "            xc1_emb2 = self.char_cnn(xc1_emb.view(-1, xc1_emb.size(2), xc1_emb.size(3)))\n",
    "            words_vecs = torch.cat((words_vecs, xc1_emb2.view(xc1.size(0), xc1.size(1), -1)), -1)\n",
    "            \n",
    "            \n",
    "        pos_global_vecs = None\n",
    "        if self.char_global_active:\n",
    "            #char_seqs = [[np.array([self._charset.get(ch, 0) for ch in token.word]) for token in graph.nodes] for graph in graphs]\n",
    "            char_global_seqs = [np.concatenate([np.append(token , np.array([1])) for token in sentence]) for sentence in char_seqs] \n",
    "            \n",
    "            pad_global_seqs = pad_sequence(char_global_seqs)\n",
    "            char_global_seqs = self.cuda_variable(torch.from_numpy(pad_global_seqs))\n",
    "            char_global_vecs = self.char_global_emb(char_global_seqs)\n",
    "            \n",
    "            #batched and char global LSTM\n",
    "            char_global_len_list = np.array([len(sentence) for sentence in char_global_seqs])\n",
    "            \n",
    "            global_indices = np.argsort(-np.array(char_global_len_list)).astype(np.int64) #sorting based on seq_len\n",
    "            char_global_len_list_ordered = char_global_len_list[global_indices] #Sorting \"char_len_list\" based on the indices\n",
    "            char_global_vecs_ordered = torch.stack([char_global_vecs[order] for order in global_indices]) #Stacking batches as an embedding based on dec order                \n",
    "            char_global_sentence_packed = nn.utils.rnn.pack_padded_sequence(char_global_vecs_ordered, char_global_len_list_ordered, batch_first=True)\n",
    "            char_global_lstm_batch_out = self.char_global_LSTM(char_global_sentence_packed)[0] # (Batch * seq_len x  2*LSTM_hidden)\n",
    "            char_global_lstm_batch_out = nn.utils.rnn.pad_packed_sequence(char_global_lstm_batch_out, batch_first=True)[0] # (Batch x seq_len x 2*LSTM_hidden)\n",
    "            char_global_lstm_batch_out = char_global_lstm_batch_out.index_select(dim=0, index=_model_var(self, torch.from_numpy(np.argsort(global_indices).astype(np.int64)))) #make it back unorder\n",
    "            \n",
    "            batch_size, word_lenth, __ = words_vecs.size()\n",
    "            char_global_feature = self.cuda_variable(torch.zeros(batch_size, word_lenth, self.char_hidden_size * self.char_directions))#self.num_pos))\n",
    "            for batch_idx in range(batch_size):\n",
    "                ch_start_idx, ch_end_idx = 0,0\n",
    "                for token_idx, token in enumerate(token_seqs[batch_idx]):\n",
    "                    token_lenth = len(token)\n",
    "                    ch_end_idx = ch_start_idx + token_lenth \n",
    "                    char_global_token = char_global_lstm_batch_out[batch_idx, ch_start_idx:ch_end_idx, :] #char_global_lstm_batch_out[0, 0:6, :] ==> *root*\n",
    "                    char_global_token_mlp = self.char_global_mlp(char_global_token)            \n",
    "                    \n",
    "                    char_global_att_score = self.att_global_score(char_global_token_mlp.transpose(0, 1))\n",
    "                    char_global_att_vec = char_global_att_score.mm(char_global_token)\n",
    "                   \n",
    "                    char_global_feature[batch_idx,token_idx,:] = char_global_att_vec #char_global_att_vec_lin\n",
    "                    ch_start_idx = ch_end_idx + 1 # +1 == white space KKL\n",
    "                    \n",
    "            #char_global_att_feature = self.char_global_linear(char_global_feature)\n",
    "            pos_global_vecs = char_global_feature\n",
    "            \n",
    "        #Processing Additional embeddings (External embedding, corpus embedding, POS)\n",
    "        if ext_word_seqs is not None:\n",
    "            ext_word_seqs = self.cuda_variable(torch.from_numpy(ext_word_seqs))\n",
    "            ext_words_vecs = self.dropout(self.ext_emb(ext_word_seqs))\n",
    "            words_vecs = torch.cat((words_vecs, ext_words_vecs.float()), -1)\n",
    "            pos_global_vecs = torch.cat((char_global_feature, ext_words_vecs.float()), -1)\n",
    "            \n",
    "        if self.dim_lang > 0:\n",
    "            lang_vecs = self.lang_emb(self.cuda_variable(torch.from_numpy(lang_seqs))) \n",
    "            words_vecs = torch.cat((words_vecs, lang_vecs), -1)\n",
    "            pos_global_vecs = torch.cat((pos_global_vecs, lang_vecs), -1)\n",
    "              \n",
    "        if self.postagger_active:\n",
    "            pos_indices = np.argsort(-np.array(lengths)).astype(np.int64) #sorting based on seq_len\n",
    "            pos_lengths = lengths[pos_indices]\n",
    "            pos_input_vecs = torch.stack([pos_global_vecs[idx] for idx in pos_indices]) #In order to put batched-LSTM: ordering inputs and stacking input_vecs = 3 x 30 x 9        \n",
    "            pos_input_vecs = nn.utils.rnn.pack_padded_sequence(pos_input_vecs, pos_lengths, batch_first=True) #padded -> pack to eliminate processes for paddings  input_vecs = lengths*batch x feature_dim = 58 x 9\n",
    "            pos_out = self.pos_LSTM(pos_input_vecs)[0] #encode_out = batch x lengths*batch x hidden_size*2 =  58x16\n",
    "            pos_out = nn.utils.rnn.pad_packed_sequence(pos_out, batch_first=True)[0] #packed -> pad #3x30x16\n",
    "            pos_out = pos_out.index_select(dim=0, index=_model_var(self, torch.from_numpy(np.argsort(pos_indices).astype(np.int64))))\n",
    "\n",
    "            pos_mlp_out = self.pos_MLP(pos_out)\n",
    "            pos_linear = self.pos_linear(pos_mlp_out)\n",
    "            pred_poses = pos_linear.data.max(2)[1].cpu()       \n",
    "            pos_seqs = self.cuda_variable(pred_poses)\n",
    "        else:\n",
    "            pos_linear = None\n",
    "            pos_seqs = self.cuda_variable(torch.from_numpy(pos_seqs)) # pos_seq = batch x seq_IDs\n",
    "\n",
    "        #pos_seqs = self.cuda_variable(pred_poses)\n",
    "        pos_vecs = self.dropout(self.pos_emb(pos_seqs)) #pos_vecs = batch x len(seq) x hidden_size = 3 x 30 x 4        \n",
    "\n",
    "        input_vecs = words_vecs\n",
    "        indices = np.argsort(-np.array(lengths)).astype(np.int64) #sorting based on seq_len\n",
    "        lengths = lengths[indices]\n",
    "        input_vecs = torch.stack([input_vecs[idx] for idx in indices]) #In order to put batched-LSTM: ordering inputs and stacking input_vecs = 3 x 30 x 9        \n",
    "        input_vecs = nn.utils.rnn.pack_padded_sequence(input_vecs, lengths, batch_first=True) #padded -> pack to eliminate processes for paddings  input_vecs = lengths*batch x feature_dim = 58 x 9\n",
    "        encode_out = self.enc_LSTM(input_vecs)[0] #encode_out = batch x lengths*batch x hidden_size*2 =  58x16\n",
    "        encode_out = nn.utils.rnn.pad_packed_sequence(encode_out, batch_first=True)[0] #packed -> pad #3x30x16\n",
    "        encode_out = encode_out.index_select(dim=0, index=_model_var(self, torch.from_numpy(np.argsort(indices).astype(np.int64))))\n",
    "\n",
    "        #MLP-based shape resizing\n",
    "        mlp_arc_rel = self.arc_MLP_rel(encode_out)\n",
    "        pos2_linear = self.pos2_linear(mlp_arc_rel)\n",
    "        pred_poses2 = pos2_linear.data.max(2)[1].cpu()\n",
    "\n",
    "        input_meta_vec = torch.cat((pos_vecs, mlp_arc_rel),-1)\n",
    "        if self.dim_lang > 0:\n",
    "            input_meta_vec = torch.cat((input_meta_vec, lang_vecs), -1)\n",
    "\n",
    "        meta_logits, meta_linear, meta_pred = self.metaLSTM(input_meta_vec, seq_lenths)\n",
    "\n",
    "        return pos_linear, pos2_linear, meta_linear #, rel_logits\n",
    "\n",
    "    def cuda_variable(self, tensor):\n",
    "        # Do cuda() before wrapping with variable\n",
    "        if torch.cuda.is_available() and self.cuda_device >= 0:\n",
    "            return Variable(tensor.cuda(self.cuda_device))\n",
    "        else:\n",
    "            return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Build the parser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLparser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_parser(self, **kwargs):\n",
    "        \n",
    "        self._verbose = kwargs.get(\"verbose\", True)\n",
    "        if self._verbose:\n",
    "            print(\"Parameters (others default):\")\n",
    "            for k in sorted(kwargs):\n",
    "                print(k, kwargs[k])\n",
    "        self._args = kwargs\n",
    "        \n",
    "        self._seed = kwargs.get(\"seed\", 0)\n",
    "        random.seed(self._seed)\n",
    "        np.random.seed(self._seed)\n",
    "        torch.manual_seed(self._seed)\n",
    "        self._cuda_device = kwargs.get(\"cuda_device\", -1)\n",
    "        self._save_model_folder = kwargs.get(\"save_model_folder\", \"../result\")\n",
    "        self._save_model_name = kwargs.get(\"save_model_name\", \"en\")\n",
    "        self._save_UDfile_folder = kwargs.get(\"save_UDfile_folder\", \"../result\")\n",
    "        self._save_UDfile_name = kwargs.get(\"save_UDfile_name\", \"en\")\n",
    "        self._model_file = kwargs.get(\"model_file\", None)\n",
    "        self._train_file = kwargs.get(\"train_file\", None)\n",
    "        self._train_lang_code = kwargs.get(\"train_lang_code\", None)\n",
    "        self._dev_file = kwargs.get(\"dev_file\", None)\n",
    "        self._dev_lang_code = kwargs.get(\"dev_lang_code\", None)\n",
    "        self._learning_rate = kwargs.get(\"learning_rate\", 0.0015)\n",
    "        self._dim_word = kwargs.get(\"dim_word\", 100)\n",
    "        self._dim_char = kwargs.get(\"dim_char\", 100) #####KKL\n",
    "        self._dim_pos = kwargs.get(\"dim_pos\", 100)\n",
    "        self._dim_rel = kwargs.get(\"dim_rel\", 30)\n",
    "        self._dim_lang = kwargs.get(\"dim_lang\", 12)\n",
    "\n",
    "        self._enc_hidden_size = kwargs.get(\"enc_hidden_size\", 400)\n",
    "        self._enc_layer_size = kwargs.get(\"enc_layer_size\", 3)\n",
    "        self._mlp_hidden_size = kwargs.get(\"mlp_hidden_size\", 300) ##before 500 KKL\n",
    "        \n",
    "        #For external-word-embedding\n",
    "        self._ext_emb_file = kwargs.get(\"ext_emb_file\", None)\n",
    "        self._ext_word_size = kwargs.get(\"ext_word_size\", 0)\n",
    "        self._dim_ext_word = kwargs.get(\"dim_ext_word\", 0)\n",
    "        self._ext_limit = kwargs.get(\"ext_limit\", 1000000)\n",
    "        self._multi_emb = kwargs.get(\"multi_emb\", False)\n",
    "        self._ext_emb = None\n",
    "        \n",
    "        self._postagger_active = kwargs.get(\"postagger_active\", True)\n",
    "        self._char_active = kwargs.get(\"char_active\", False)\n",
    "        self._char_global_active = kwargs.get(\"char_global_active\", True)\n",
    "        self._elmo_active = kwargs.get(\"elmo_active\", False)\n",
    "        self._elmo_weight_file = kwargs.get(\"elmo_weight_file\", None)\n",
    "        self._elmo_option_file = kwargs.get(\"elmo_option_file\", None)\n",
    "\n",
    "        if self._model_file is None:\n",
    "            print(\"########## create a new tagger ##########\")\n",
    "            # Init external embedding       \n",
    "            self._ext_vocab = None\n",
    "            self._init_ext_embeddings(self._ext_emb_file)\n",
    "            # Init Voca\n",
    "            self.build_vocab(self._train_file, self._train_lang_code) # Init Vocaburaries and set dictionaries\n",
    "            self._load_dataset(self._train_file, self._train_lang_code, self._dev_file, self._dev_lang_code)\n",
    "            \n",
    "        self._init_model()        # Init generate parser_model\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def _init_ext_embeddings(self, ext_emb_file=None):\n",
    "        # set external word embeddings\n",
    "        \n",
    "        if ext_emb_file is not None:\n",
    "            external_embedding_fp = open(ext_emb_file, 'rb')#, encoding=\"ISO-8859-1\")\n",
    "            external_embedding_fp.readline() #Caution!! trow out first line of the embedding\n",
    "            #self.external_embedding = {line.decode('utf8').split(' ')[0]: [float(f) for f in line.decode('utf8').strip().split(' ')[1:]] for line, idx in zip(external_embedding_fp, range(self._ext_limit))}\n",
    "\n",
    "            error_lines =0\n",
    "            self.external_embedding = {}\n",
    "            for l, idx in zip(external_embedding_fp, range(self._ext_limit)):\n",
    "                try:\n",
    "                    line = l.decode('utf8')\n",
    "                    line_split = line.strip().split(' ')\n",
    "                    self.external_embedding.update({line_split[0]: [float(f) for f in line_split[1:]]})\n",
    "                except:\n",
    "                    error_lines += 1\n",
    "                    continue\n",
    "\n",
    "            external_embedding_fp.close()\n",
    "            print(\"# of embedding reading error: \", error_lines)\n",
    "            \n",
    "            self._dim_ext_word  = len(list(self.external_embedding.values())[0])\n",
    "            self._args['dim_ext_word'] = self._dim_ext_word\n",
    "            \n",
    "            self._ext_vocab = {word: i + 3 for i, word in enumerate(self.external_embedding)}\n",
    "            self._ext_word_size = len(self._ext_vocab)\n",
    "            self._args['ext_word_size'] = self._ext_word_size\n",
    "            \n",
    "            self._ext_emb = np.zeros((len(self.external_embedding) + 3, self._dim_ext_word))\n",
    "            for word, i in self._ext_vocab.items():\n",
    "                if len(self.external_embedding[word]) == self._dim_ext_word:\n",
    "                    self._ext_emb[i] = self.external_embedding[word]\n",
    "                else:\n",
    "                    print(\"word embedding is not matched dimensions, an error [voca]:\", word)\n",
    "            \n",
    "            self._ext_vocab['*pad*'] = 0\n",
    "            self._ext_vocab['*root*'] = 1\n",
    "            print('Loaded external embedding. Vector dimensions ', self._dim_ext_word, \"number of external words\", self._ext_word_size)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def _init_model(self):\n",
    "\n",
    "        self.encode_model = Encode_model(len(self._vocab),\n",
    "                                         self._dim_word,\n",
    "                                         len(self._charset),\n",
    "                                         self._dim_char,\n",
    "                                         len(self._upos),\n",
    "                                         self._dim_pos,\n",
    "                                         len(self._rels),\n",
    "                                         self._dim_rel,\n",
    "                                         len(self._langs),\n",
    "                                         self._dim_lang,\n",
    "                                         self._ext_emb,\n",
    "                                         self._ext_word_size,\n",
    "                                         self._dim_ext_word,\n",
    "                                         self._enc_hidden_size,\n",
    "                                         self._enc_layer_size,\n",
    "                                         self._mlp_hidden_size,\n",
    "                                         self._char_active,\n",
    "                                         self._char_global_active,\n",
    "                                         self._elmo_active,\n",
    "                                         self._postagger_active,\n",
    "                                         self._elmo_weight_file,\n",
    "                                         self._elmo_option_file,\n",
    "                                         self._cuda_device\n",
    "                                        )\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.encode_model.parameters()),\n",
    "                                     lr=self._learning_rate, betas=(0.9, 0.99), eps=1e-12)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(self.encode_model.parameters(), max_norm=5.0)\n",
    "\n",
    "        class Annealing(object):\n",
    "\n",
    "            def __init__(self, optimizer, lr):                \n",
    "                self.step = 0\n",
    "                self.lr = lr\n",
    "                self.optimizer = optimizer\n",
    "\n",
    "            def __call__(self):\n",
    "                self.step = self.step + 1\n",
    "                decay, decay_step = 0.75, 5000\n",
    "                decay_rate = decay ** (self.step / decay_step)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr * decay_rate\n",
    "\n",
    "        self.annealing = Annealing(optimizer, self._learning_rate)\n",
    "        \n",
    "        if torch.cuda.is_available() and self._cuda_device >= 0:\n",
    "            self.encode_model.cuda(self._cuda_device)\n",
    "        \n",
    "        return self\n",
    "\n",
    "       \n",
    "    def _load_dataset(self, train_UD=None, train_lang_code=None, dev_UD=None, dev_lang_code=None):\n",
    "\n",
    "        if train_UD is not None:\n",
    "            if isinstance(train_UD, str):\n",
    "                self.train_graphs_list = read_conll(train_UD, train_lang_code)\n",
    "            elif isinstance(train_UD, list):\n",
    "                self.train_graphs_list = []\n",
    "                for file, lang_code in zip(train_UD, train_lang_code):\n",
    "                    self.train_graphs_list.extend(read_conll(file, lang_code))\n",
    "        else:\n",
    "            self.train_graphs_list = None\n",
    "\n",
    "\n",
    "        if dev_UD is not None:\n",
    "            self.dev_graphs_list = read_conll(dev_UD, dev_lang_code)\n",
    "        else:\n",
    "            self.dev_graphs_list = None\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _load_vocab(self, vocab):\n",
    "        self._fullvocab = vocab\n",
    "        self._upos = {p: i for i, p in enumerate(vocab[\"upos\"])}\n",
    "        self._iupos = vocab[\"upos\"]\n",
    "        self._xpos = {p: i for i, p in enumerate(vocab[\"xpos\"])}\n",
    "        self._ixpos = vocab[\"xpos\"]\n",
    "        self._vocab = {w: i + 3 for i, w in enumerate(vocab[\"vocab\"])}\n",
    "        self._wordfreq = vocab[\"wordfreq\"]\n",
    "        self._charset = {c: i + 3 for i, c in enumerate(vocab[\"charset\"])}\n",
    "        self._charfreq = vocab[\"charfreq\"]\n",
    "        self._rels = {r: i for i, r in enumerate(vocab[\"rels\"])}\n",
    "        self._irels = vocab[\"rels\"]\n",
    "        self._feats = {f: i + 1 for i, f in enumerate(vocab[\"feats\"])}\n",
    "        \n",
    "        self._langs = {r: i+2 for i, r in enumerate(vocab[\"lang\"])}\n",
    "        self._ilangs = vocab[\"lang\"]\n",
    "        \n",
    "        self._vocab['*pad*'] = 0\n",
    "        self._charset['*pad*'] = 0\n",
    "        self._langs['*pad*'] = 0\n",
    "\n",
    "        self._vocab['*root*'] = 1\n",
    "        self._charset['*root*'] = 1\n",
    "        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    def load_vocab(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        self._load_vocab(vocab)\n",
    "        return self\n",
    "\n",
    "    def save_vocab(self, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self._fullvocab, f)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def build_vocab(self, filename=None, lang_code=None, savefile=None, cutoff=1):\n",
    "        if filename is None:\n",
    "            filename = self._train_file\n",
    "        if isinstance(filename, str):\n",
    "            graphs = read_conll(filename, lang_code)\n",
    "        elif isinstance(filename, list):\n",
    "            graphs = []\n",
    "            for f,l in zip(filename, lang_code):\n",
    "                graphs.extend(read_conll(f, l))\n",
    "\n",
    "        self._fullvocab= buildVocab(graphs, cutoff)\n",
    "\n",
    "        if savefile:\n",
    "            self.save_vocab(savefile)\n",
    "        self._load_vocab(self._fullvocab)\n",
    "        return self\n",
    "       \n",
    "    \n",
    "    def save(self, filename=None):\n",
    "        if filename is None:\n",
    "            filename = self._save_model_folder+\"/\"+self._save_model_name\n",
    "        self.save_vocab(filename + \".vocab\")\n",
    "        with open(filename + \".params\", \"wb\") as f:\n",
    "            pickle.dump((self._args, self._ext_vocab), f)\n",
    "            \n",
    "        tmp = filename + '.model'\n",
    "        torch.save(self.encode_model.state_dict(), tmp)\n",
    "        #shutil.move(tmp, filename)\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def load(self, model_name, **kwargs):\n",
    "        self.load_vocab(model_name + \".vocab\")\n",
    "        kwargs[\"model_file\"] = model_name+\".model\"\n",
    "        with open(model_name + \".params\", \"rb\") as f:\n",
    "            (args, self._ext_vocab)= pickle.load(f)\n",
    "            args.update(kwargs)\n",
    "            self.create_parser(**args)\n",
    "        \n",
    "        if kwargs[\"cuda_device\"] == -1:  \n",
    "            self.encode_model.load_state_dict(torch.load(model_name + \".model\", map_location='cpu'))\n",
    "        else:\n",
    "            self.encode_model.load_state_dict(torch.load(model_name + \".model\"))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, y, t):\n",
    "        \n",
    "        arc_logits = y\n",
    "        true_arcs = t\n",
    "        \n",
    "        b, l1, l2 = arc_logits.size()\n",
    "        true_arcs = pad_sequence(true_arcs, padding=-1, dtype=np.int64)\n",
    "        true_arcs = self.cuda_variable(torch.from_numpy(true_arcs))\n",
    "        \n",
    "        arc_loss = F.cross_entropy(arc_logits.view(b * l1, l2), true_arcs.view(b * l1), ignore_index=-1)\n",
    "        loss = arc_loss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def cuda_variable(self, tensor):\n",
    "        # Do cuda() before wrapping with variable\n",
    "        if torch.cuda.is_available() and self._cuda_device >= 0:\n",
    "            return Variable(tensor.cuda(self._cuda_device))\n",
    "        else:\n",
    "            return Variable(tensor)\n",
    "    \n",
    "    \n",
    "    def compute_accuracy(self, y, t):\n",
    "        \n",
    "        arc_logits = y\n",
    "        true_arcs = t\n",
    "\n",
    "        if type(arc_logits) == np.ndarray: \n",
    "            arc_logits = torch.from_numpy(arc_logits)\n",
    "            pred_arcs = arc_logits.max(2)[1].cpu()\n",
    "        else: pred_arcs = arc_logits.data.max(2)[1].cpu()\n",
    "        b, l1, l2 = arc_logits.size()\n",
    "        true_arcs = pad_sequence(true_arcs, padding=-1, dtype=np.int64)\n",
    "        true_arcs = torch.from_numpy(true_arcs)\n",
    "        \n",
    "        correct = pred_arcs[:,1:].eq(true_arcs[:,1:]).cpu().sum()\n",
    "        num_tokens = (b * l1 - np.sum(true_arcs.cpu().numpy() == -1))  # -b for excluding ROOT\n",
    "        \n",
    "        return correct, num_tokens\n",
    "    \n",
    "    def compute_LAS(self, arc_y, arc_t, rel_y, rel_t):\n",
    "        \n",
    "        arc_logits = arc_y\n",
    "        true_arcs = arc_t\n",
    "        rel_logits = rel_y\n",
    "        true_rels = rel_t\n",
    "        \n",
    "        pred_arcs = arc_logits.data.max(2)[1].cpu()\n",
    "        b, l1, l2 = arc_logits.size()\n",
    "        true_arcs = pad_sequence(true_arcs, padding=-1, dtype=np.int64)\n",
    "        true_arcs = torch.from_numpy(true_arcs)\n",
    "        \n",
    "        arc_correct_idx = pred_arcs[:,1:].eq(true_arcs[:,1:])\n",
    "        \n",
    "        pred_rels = rel_logits.data.max(2)[1].cpu()\n",
    "        b, l1, l2 = rel_logits.size()\n",
    "        true_rels = pad_sequence(true_rels, padding=-1, dtype=np.int64)\n",
    "        true_rels = torch.from_numpy(true_rels)\n",
    "        \n",
    "        rel_correct_idx = pred_rels[:,1:].eq(true_rels[:,1:])\n",
    "        arc_rel_match = arc_correct_idx + rel_correct_idx #arc_correct_idx.eq(rel_correct_idx)\n",
    "        \n",
    "        las_correct = arc_rel_match.eq(2).cpu().sum()\n",
    "        uas_correct = pred_arcs[:,1:].eq(true_arcs[:,1:]).cpu().sum()\n",
    "        num_tokens = (b * l1 - np.sum(true_arcs.cpu().numpy() == -1))  # -b for excluding ROOT\n",
    "        \n",
    "        return las_correct, uas_correct, num_tokens\n",
    "    \n",
    "    def compute_test_LAS(self, arc_y, arc_t, rel_y, rel_t):\n",
    "        \n",
    "        pred_arcs = np.array(arc_y)\n",
    "        true_arcs = arc_t\n",
    "        pred_rels = np.array(rel_y)\n",
    "        true_rels = rel_t\n",
    "        \n",
    "        pred_arcs = pad_sequence(pred_arcs, padding=-2, dtype=np.int64)\n",
    "        true_arcs = pad_sequence(true_arcs, padding=-1, dtype=np.int64)\n",
    "        pred_rels = pad_sequence(pred_rels, padding=-2, dtype=np.int64)\n",
    "        true_rels = pad_sequence(true_rels, padding=-1, dtype=np.int64)\n",
    "        \n",
    "        pred_arcs = torch.from_numpy(pred_arcs)\n",
    "        true_arcs = torch.from_numpy(true_arcs)\n",
    "        pred_rels = torch.from_numpy(pred_rels)\n",
    "        true_rels = torch.from_numpy(true_rels)\n",
    "        \n",
    "        b, l1 = pred_arcs.size()\n",
    "        \n",
    "        arc_correct_idx = pred_arcs[:,1:].eq(true_arcs[:,1:])\n",
    "        rel_correct_idx = pred_rels[:,1:].eq(true_rels[:,1:])\n",
    "        arc_rel_match = arc_correct_idx + rel_correct_idx\n",
    "\n",
    "        las_correct = arc_rel_match.eq(2).cpu().sum()\n",
    "        uas_correct = pred_arcs[:,1:].eq(true_arcs[:,1:]).cpu().sum()\n",
    "        num_tokens = (b * l1 - np.sum(true_arcs.cpu().numpy() == -1))  # -b for excluding ROOT\n",
    "        \n",
    "        return las_correct, uas_correct, num_tokens\n",
    "    \n",
    "    \n",
    "    def train(self, lang_code=None, train_file=None, batch_size=32):\n",
    "        \n",
    "        self.encode_model.train() # set train mode for dropout on\n",
    "        total_sentence = 0 # of sentence\n",
    "        total_loss = 0 # loss for a ephoc\n",
    "        total_token = 0 # of tokens\n",
    "        total_uas_correct = 0   \n",
    "        total_las_correct = 0\n",
    "        total_pos_correct = 0\n",
    "        \n",
    "        graphs_list = self.train_graphs_list if train_file is None else read_conll(train_file, lang_code)\n",
    "        shuffledTrain = graphs_list\n",
    "        random.shuffle(shuffledTrain) ###KKL\n",
    "        num_sentence = len(shuffledTrain)\n",
    "        num_batch = int(np.ceil(num_sentence/batch_size))\n",
    "        \n",
    "        for idx in range(num_batch):  \n",
    "            graphs = shuffledTrain[idx*batch_size : idx*batch_size+batch_size] #takes a training corpus with batch_size\n",
    "            \n",
    "            self.annealing.optimizer.zero_grad()\n",
    "\n",
    "            seq_lenths = np.array([len(graph.nodes) for graph in graphs])\n",
    "            word_seqs = [np.array([ self._vocab.get(token.norm, 0) if self._dim_word > 2 else 0 for token in graph.nodes ]) for graph in graphs]\n",
    "            pos_seqs = [np.array([ self._upos.get(token.upos, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "            if self._ext_emb_file is not None:\n",
    "                if self._multi_emb:\n",
    "                    ext_word_seqs = [np.array([ self._ext_vocab.get(token.lang.split('_')[0]+\":\"+token.norm, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "                else:\n",
    "                    ext_word_seqs = [np.array([ self._ext_vocab.get(token.norm, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "            else:\n",
    "                ext_word_seqs = None\n",
    "                    \n",
    "            char_seqs = [[np.array([self._charset.get(ch, 0) for ch in token.word]) for token in graph.nodes] for graph in graphs] if (self._char_active or self._char_global_active) else None\n",
    "            token_seqs = [np.array([ token.word for token in graph.nodes ]) for graph in graphs]\n",
    "            lang_seqs = [np.array([ self._langs.get(token.lang, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "            \n",
    "            #padding\n",
    "            word_seqs_pad = pad_sequence(word_seqs)\n",
    "            pos_seqs_pad = pad_sequence(pos_seqs)\n",
    "            ext_word_seqs_pad = pad_sequence(ext_word_seqs) if self._ext_emb_file is not None else None\n",
    "            lang_seqs_pad = pad_sequence(lang_seqs)\n",
    "            \n",
    "            poses, poses2, meta_poses = self.encode_model(word_seqs_pad, pos_seqs_pad, ext_word_seqs_pad, char_seqs, token_seqs, lang_seqs_pad, seq_lenths, train=True)\n",
    "            pos_logits = poses\n",
    "            pos2_logits = poses2\n",
    "            \n",
    "            true_poses = np.array([np.array([self._upos.get(token.upos, -1) for token in graph.nodes]) for graph in graphs])    \n",
    "            loss_poses = self.compute_loss(pos_logits, true_poses) if self._postagger_active else 0\n",
    "            loss_poses2 = self.compute_loss(pos2_logits, true_poses) if self._postagger_active else 0\n",
    "            loss_meta_poses = self.compute_loss(meta_poses, true_poses) if self._postagger_active else 0\n",
    "\n",
    "            loss = loss_poses + loss_poses2 + loss_meta_poses\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pos_correct, pos_num_token = self.compute_accuracy(meta_poses, true_poses) if self._postagger_active else (0,0)           \n",
    "            total_pos_correct += pos_correct\n",
    "            total_token += pos_num_token\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(self.encode_model.parameters(), 5.0)\n",
    "            self.annealing.optimizer.step()\n",
    "            self.annealing.__call__()\n",
    "            total_sentence += len(graphs)\n",
    "            \n",
    "        pos_accuracy = round(float(total_pos_correct)/float(total_token)*100, 2) if self._postagger_active else None\n",
    "  \n",
    "        print(total_sentence,\"trained\", \"###### total Loss: \", total_loss ,\"POS: \", pos_accuracy)      \n",
    "        return pos_accuracy\n",
    "\n",
    "        \n",
    "    def test(self, lang_code=None, test_file=None, gold_file=None, batch_size=32, write_file=False, epoch=0, file_location=None):\n",
    "        \n",
    "        self.encode_model.eval()\n",
    "        total_sentence = 0 # of sentence\n",
    "        total_token = 0 # of tokens\n",
    "        total_uas_correct = 0  \n",
    "        total_las_correct = 0\n",
    "        total_pos_correct = 0\n",
    "        total_pos_char_correct = 0\n",
    "        total_pos_meta_correct = 0\n",
    "        \n",
    "        graphs_list = copy.deepcopy(self.dev_graphs_list) if test_file is None else read_conll(test_file, lang_code=lang_code)\n",
    "        num_sentence = len(graphs_list)\n",
    "        num_batch = int(np.ceil(num_sentence/batch_size))\n",
    "    \n",
    "        for idx in range(num_batch):\n",
    "            graphs = graphs_list[idx*batch_size:idx*batch_size+batch_size] #takes a training corpus with batch_size\n",
    "            \n",
    "            #print(graphs)\n",
    "            #converts as a list of vectors and pads based on batch_size\n",
    "            seq_lenths = np.array([len(graph.nodes) for graph in graphs])\n",
    "            word_seqs = [np.array([ self._vocab.get(token.norm, 0) if self._dim_word > 2 else 0 for token in graph.nodes ]) for graph in graphs]\n",
    "            pos_seqs = [np.array([ self._upos.get(token.upos, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "            if self._ext_emb_file is not None:\n",
    "                if self._multi_emb:\n",
    "                    ext_word_seqs = [np.array([ self._ext_vocab.get(token.lang.split('_')[0]+\":\"+token.norm, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "                else:\n",
    "                    ext_word_seqs = [np.array([ self._ext_vocab.get(token.norm, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "            else:\n",
    "                ext_word_seqs = None\n",
    "                \n",
    "            char_seqs = [[np.array([self._charset.get(ch, 0) for ch in token.word]) for token in graph.nodes] for graph in graphs] if (self._char_active or self._char_global_active) else None\n",
    "            token_seqs = [np.array([ token.word for token in graph.nodes ]) for graph in graphs]\n",
    "            lang_seqs = [np.array([ self._langs.get(token.lang, 0) for token in graph.nodes ]) for graph in graphs]\n",
    "\n",
    "            word_seqs_pad = pad_sequence(word_seqs)\n",
    "            pos_seqs_pad = pad_sequence(pos_seqs)\n",
    "            ext_word_seqs_pad = pad_sequence(ext_word_seqs) if self._ext_emb_file is not None else None\n",
    "            lang_seqs_pad = pad_sequence(lang_seqs)\n",
    "                \n",
    "            poses, poses2, meta_poses = self.encode_model(word_seqs_pad, pos_seqs_pad, ext_word_seqs_pad, char_seqs, token_seqs, lang_seqs_pad, seq_lenths, train=True)\n",
    "            pos_logits = poses\n",
    "            pos2_logits = poses2\n",
    "            \n",
    "            true_poses = np.array([np.array([self._upos.get(token.upos, -1) for token in graph.nodes]) for graph in graphs])    \n",
    "            pos_correct, pos_num_token = self.compute_accuracy(pos2_logits, true_poses) if self._postagger_active else (0,0)           \n",
    "            pos_char_correct, pos_num_token = self.compute_accuracy(pos_logits, true_poses) if self._postagger_active else (0,0)           \n",
    "            pos_meta_correct, pos_num_token = self.compute_accuracy(meta_poses, true_poses) if self._postagger_active else (0,0)           \n",
    "\n",
    "            total_pos_correct += pos_correct\n",
    "            total_pos_char_correct += pos_char_correct\n",
    "            total_pos_meta_correct += pos_meta_correct\n",
    "            total_token += pos_num_token\n",
    "            total_sentence += len(graphs)\n",
    "            \n",
    "            if write_file:\n",
    "            \n",
    "                #set the predicted posses to each sentence\n",
    "                if self._postagger_active:\n",
    "                    pred_poses = meta_poses.data.max(2)[1].cpu()\n",
    "                    batch_idx=0\n",
    "                    for graph in graphs:              \n",
    "                        token_idx=0\n",
    "                        for token in graph.nodes:\n",
    "                            token.upos = self._iupos[pred_poses[batch_idx,token_idx]]\n",
    "                            token_idx +=1\n",
    "                        batch_idx += 1\n",
    "\n",
    "        pos_accuracy = round(float(total_pos_correct)/float(total_token)*100, 2) if self._postagger_active else None\n",
    "        pos_char_accuracy = round(float(total_pos_char_correct)/float(total_token)*100, 2) if self._postagger_active else None\n",
    "        pos_meta_accuracy = round(float(total_pos_meta_correct)/float(total_token)*100, 2) if self._postagger_active else None\n",
    "\n",
    "        print(total_sentence,\"Tested\", \"POS_meta\",pos_meta_accuracy, \"POS: \", pos_accuracy, \"POS_char:\", pos_char_accuracy)      \n",
    "        \n",
    "        if write_file:\n",
    "            \n",
    "            write_loca = file_location if file_location is not None else self._save_UDfile_folder+self._save_UDfile_name+str(epoch)+\"conllu\"\n",
    "            write_conll(write_loca, graphs_list)\n",
    "            if gold_file is not None:\n",
    "                os.system('python ./evaluation/conll18_ud_eval.py -v '+gold_file+' '+write_loca+' > '+write_loca+'.txt')\n",
    "        \n",
    "        return pos_meta_accuracy\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Finish line of MLparser (class) ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_list(filename=\"./train_list.csv\"):\n",
    "    line_counter = 0\n",
    "    data_header = []\n",
    "    train_list = []\n",
    "\n",
    "    with open(filename) as train_data:\n",
    "        while True:\n",
    "            data = train_data.readline()\n",
    "            if not data: break\n",
    "            if line_counter == 0:\n",
    "                data_header = data.split(\",\")\n",
    "            else:\n",
    "                train_list.append(data.split(\",\"))\n",
    "            line_counter +=1\n",
    "    print(\"Header: \\t\", data_header)\n",
    "    \n",
    "    return train_list\n",
    "\n",
    "    \n",
    "def test_model(model, test_file, test_lang, save_folder, save_name, batch_size, elmo_weight=None, elmo_option=None, ensemble=False, cuda_device=-1, file_location=None):\n",
    "    \n",
    "    if ensemble:\n",
    "        print(\"Ensemble is not implimented yet\")\n",
    "        \n",
    "    else:\n",
    "        mtparser = MLparser()\n",
    "        mtparser.load(model_name=model,\n",
    "                      save_UDfile_folder=save_folder,\n",
    "                      save_UDfile_name=save_name,\n",
    "                      elmo_weight_file = elmo_weight,\n",
    "                      elmo_option_file = elmo_option,\n",
    "                      cuda_device=cuda_device)\n",
    "\n",
    "        accuracy = mtparser.test(lang_code=test_lang,\n",
    "                          test_file=test_file,\n",
    "                          batch_size=batch_size,\n",
    "                          file_location=file_location,\n",
    "                          write_file=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: \t ['language_ud_full', 'language_ud', 'language', 'language_code', 'train_size', 'dev_size', 'train', 'max_epoch', 'train_files', 'dev_file', 'train_language_codes', 'char', 'pos', 'train_type\\n']\n",
      "A corpus loaded: ['UD_Chinese-GSD', 'UD_Chinese', 'Chinese', 'zh_gsd', '3997', '0.7', 'yes', '280', 'zh_gsd-ud-train.conllu', 'zh_gsd-ud-dev.conllu', 'zh_gsd', 'yes', 'yes', '\\n']\n",
      "Parameters (others default):\n",
      "char_active True\n",
      "char_global_active True\n",
      "cuda_device -1\n",
      "dev_file /Users/lim/lattice_parser/corpus/release-2.2-st-train-dev-data/ud-treebanks-v2.2/UD_Chinese-GSD/zh_gsd-ud-dev.conllu\n",
      "dev_lang_code zh_gsd\n",
      "dim_lang 0\n",
      "dim_word 100\n",
      "elmo_active False\n",
      "elmo_option_file /Users/lim/lattice_parser/ELMO/Chinese/options.json\n",
      "elmo_weight_file /Users/lim/lattice_parser/ELMO/Chinese/weights.hdf5\n",
      "ext_limit 1500000\n",
      "learning_rate 0.002\n",
      "multi_emb False\n",
      "postagger_active True\n",
      "save_UDfile_folder /Users/lim/lattice_parser/result/UD_Chinese-GSD/\n",
      "save_UDfile_name Chinese\n",
      "save_model_folder /Users/lim/lattice_parser/result/UD_Chinese-GSD/\n",
      "save_model_name Chinese\n",
      "seed 47\n",
      "train_file /Users/lim/lattice_parser/corpus/release-2.2-st-train-dev-data/ud-treebanks-v2.2/UD_Chinese-GSD/zh_gsd-ud-train.conllu\n",
      "train_lang_code zh_gsd\n",
      "########## create a new tagger ##########\n",
      "read_conll with zh_gsd\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9b881880e676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m                                        \u001b[0mcuda_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                        \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                                        \u001b[0mmulti_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                                       )\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ccf33ea85f99>\u001b[0m in \u001b[0;36mcreate_parser\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_ext_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ext_emb_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# Init Voca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_lang_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Init Vocaburaries and set dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_lang_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dev_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dev_lang_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ccf33ea85f99>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, filename, lang_code, savefile, cutoff)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-307e24158125>\u001b[0m in \u001b[0;36mread_conll\u001b[0;34m(filename, lang_code)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0msentence_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-307e24158125>\u001b[0m in \u001b[0;36mget_graph\u001b[0;34m(graphs, words, tokens, edges)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDependencyGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-778307e0fb06>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, words, tokens)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*root*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*root*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    \n",
    "    import argparse\n",
    "    import sys\n",
    "    \n",
    "    sys.argv = ['-f'] + [\"train\"] + [\"--home_dir=/Users/lim/lattice_parser/\"]\n",
    "\n",
    "    # Parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"mode\", default=\"train\", type=str, help=\"train? or test?\")\n",
    "    parser.add_argument(\"--cuda_device\", default=-1, type=int, help=\"-1: without GPU, 0: with GPU,     e.g) CUDA_VISIBLE_DEVICES=4 python tagger.py --cuda_device=0\")\n",
    "    parser.add_argument(\"--home_dir\", default=\"/home/test/tagger/\", type=str, help=\"homedirectory. i.g.) /home/abc/tagger/\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size.\")\n",
    "    \n",
    "    ##For Training options\n",
    "    parser.add_argument(\"--ud_name\", default=None, type=str, help=\"UD language name    e.g)UD_English-EWT\")\n",
    "    parser.add_argument(\"--min_epochs\", default=80, type=int, help=\"# of epochs to start saving model\")\n",
    "    parser.add_argument(\"--epochs\", default=300, type=int, help=\"Total number of epochs\")\n",
    "    parser.add_argument(\"--char_tok_active\", default=True, help=\"Activate token-based character emb.\")\n",
    "    parser.add_argument(\"--elmo_active\", default=False, help=\"Activate ELMO emb\")\n",
    "\n",
    "    ##For Testing options\n",
    "    parser.add_argument(\"--test_lang_code\", default=None, type=str, help=\"Language_code    e.g)en_ewt\")\n",
    "    parser.add_argument(\"--model_file\", default=None, type=str, help=\"Location of the model. Do not add .model    e.g)./result/UD_English-EWT/English07_95.17\")\n",
    "    parser.add_argument(\"--test_file\", default=None, type=str, help=\"Location of the test corpus.     e.g)./corpus/en-dev.conllu\")\n",
    "    parser.add_argument(\"--gold_file\", default=None, type=str, help=\"Location of the gold corpus.     e.g)./corpus/en-test.conllu\")\n",
    "    parser.add_argument(\"--out_file\", default=None, type=str, help=\"Location of the tagging output.   e.g)./corpus/en.out\")\n",
    "    parser.add_argument(\"--elmo_weight\", default=None, type=str, help=\"Location of the ELMo weight.   e.g)./ELMO/English/weights.hdf5\")\n",
    "    parser.add_argument(\"--elmo_option\", default=None, type=str, help=\"Location of the ELMo option.   e.g)./ELMO/English/options.json\")\n",
    "    \n",
    "    # Load defaults\n",
    "    args = parser.parse_args()\n",
    "    home_dir = args.home_dir \n",
    "    \n",
    "    if args.mode==\"predict\":    \n",
    "\n",
    "        test_lang = \"UD_English-EWT\" if args.ud_name is None else args.ud_name\n",
    "        test_lang_code = \"en_ewt\" if args.test_lang_code is None else args.test_lang_code\n",
    "        model_file = home_dir+\"/result/UD_English/English1_17.65\" if args.model_file is None else args.model_file\n",
    "        test_file = home_dir+\"/corpus/official-submissions/conll18-baseline/en_ewt.conllu\" if args.test_file is None else args.test_file\n",
    "        gold_file = home_dir+\"/corpus/official-submissions/00-gold-standard/en_ewt.conllu\" if args.gold_file is None else args.gold_file\n",
    "        elmo_weight = home_dir+\"/ELMO/\"+test_lang+\"/weights.hdf5\" if args.elmo_weight is None else args.elmo_weight\n",
    "        elmo_option = home_dir+\"/ELMO/\"+test_lang+\"/options.json\" if args.elmo_option is None else args.elmo_option\n",
    "        file_location= \"./\"+test_lang+\".eval2\" if args.out_file is None else args.out_file\n",
    "        cuda_device = args.cuda_device\n",
    "\n",
    "\n",
    "        acc = test_model(model=model_file,\n",
    "                         test_file=test_file,\n",
    "                         test_lang=test_lang_code,\n",
    "                         save_folder=\"../../result/\",\n",
    "                         save_name=\"test\",\n",
    "                         file_location=file_location,\n",
    "                         elmo_option = elmo_option,\n",
    "                         elmo_weight = elmo_weight,\n",
    "                         batch_size=args.batch_size,\n",
    "                         cuda_device=cuda_device,\n",
    "                         ensemble=False)\n",
    "        os.system('python ./evaluation/conll18_ud_eval.py -v '+gold_file+' '+\"./\"+test_lang+\".eval2 > \"+ test_lang+'.txt')\n",
    "\n",
    "\n",
    "    elif args.mode==\"train\":\n",
    "        \n",
    "        corpus_list = read_corpus_list(\"./train_list.csv\")\n",
    "        for corpus in corpus_list:\n",
    "            if corpus[6] == \"no\": continue\n",
    "            else:\n",
    "                print(\"A corpus loaded:\", corpus)\n",
    "    \n",
    "                language = corpus[2] \n",
    "                train_lang_code = corpus[3] \n",
    "                dev_lang_code = corpus[3]\n",
    "                test_lang_code = corpus[3]\n",
    "                \n",
    "                elmo_active = args.elmo_active\n",
    "                char_tok_active = False if elmo_active else args.char_tok_active \n",
    "\n",
    "                home_dir = home_dir\n",
    "                save_home = home_dir+\"result/\"+corpus[0]+\"/\" \n",
    "                corpus_home = home_dir+\"corpus/\"+\"release-2.2-st-train-dev-data/ud-treebanks-v2.2/\" \n",
    "                emb_home = home_dir+\"embeddings/\"+language+\"/\"\n",
    "                emb_file = train_lang_code.split('_')[0] +\".vectors\"\n",
    "\n",
    "                test_file = home_dir+\"corpus/official-submissions/Uppsala-18/\"+test_lang_code+\".conllu\" ##KKL\n",
    "                gold_file = home_dir+\"corpus/official-submissions/00-gold-standard/\"+test_lang_code+\".conllu\"\n",
    "\n",
    "                train_file = corpus[8]\n",
    "                dev_file = corpus[9]\n",
    "\n",
    "                #for training and testing\n",
    "                mlparser = MLparser()\n",
    "                mlparser.create_parser(#train_file=[corpus_home+corpus[0]+\"/\"+train_file, corpus_home+\"/UD_English-GUM/en_gum-ud-train.conllu\", corpus_home+\"/UD_English-LinES/en_lines-ud-train.conllu\"],\n",
    "                                       #train_lang_code =[train_lang_code, \"en_gum\", \"en_lines\"],\n",
    "                                       train_file=corpus_home+corpus[0]+\"/\"+train_file,\n",
    "                                       train_lang_code = train_lang_code,\n",
    "                                       dev_file=corpus_home+corpus[0]+\"/\"+dev_file,\n",
    "                                       dev_lang_code = dev_lang_code,\n",
    "                                       save_model_folder=save_home,\n",
    "                                       save_model_name=language,\n",
    "                                       save_UDfile_folder=save_home,\n",
    "                                       save_UDfile_name=language,\n",
    "                                       #ext_emb_file=emb_home+emb_file,\n",
    "                                       char_active=char_tok_active,\n",
    "                                       char_global_active=True,\n",
    "                                       elmo_active=elmo_active,\n",
    "                                       postagger_active=True,\n",
    "                                       elmo_weight_file=home_dir+\"ELMO/\"+corpus[2]+\"/weights.hdf5\",\n",
    "                                       elmo_option_file=home_dir+\"ELMO/\"+corpus[2]+\"/options.json\",\n",
    "                                       seed=random.randrange(100),\n",
    "                                       dim_lang=0,\n",
    "                                       ext_limit=1500000,\n",
    "                                       dim_word=100,\n",
    "                                       cuda_device=args.cuda_device,\n",
    "                                       learning_rate=0.002,\n",
    "                                       multi_emb=False\n",
    "                                      )\n",
    "\n",
    "\n",
    "                batch_size = args.batch_size\n",
    "                num_train_epoch = int(corpus[7]) # epoch num\n",
    "                best_accuracy = 0\n",
    "                best_epoch = 0\n",
    "                min_test_epoch = round(num_train_epoch/7) ## minimun training epoch without testing\n",
    "                force_save_per = 20 ## force to save a model per\n",
    "                req_save_epoch = 1  ## save model req_save_epoch times before the end of training, it should be bigger than 0!!\n",
    "                save_epoch = copy.deepcopy(req_save_epoch) ##it should be bigger than 0!!\n",
    "                stop_run = 300   ## stop run when non_update_count is over\n",
    "                non_update_count = 0  ## count on the number of non-update\n",
    "\n",
    "                print(\"batch_size:\", batch_size)\n",
    "\n",
    "                for epoch in range(num_train_epoch):\n",
    "\n",
    "                    start = time.time()\n",
    "                    if non_update_count > stop_run or save_epoch < 1:\n",
    "                        break\n",
    "\n",
    "                    pos_acc = mlparser.train(batch_size=batch_size)\n",
    "                    if pos_acc > 101:  ## if traning accuracy are higher than 99.8 then save next save_epoc times and break\n",
    "                        save_epoch-=1\n",
    "\n",
    "                    if epoch > min_test_epoch: \n",
    "                        non_update_count+=1\n",
    "                        pos_acc = mlparser.test(batch_size=batch_size, write_file=False, epoch=epoch)\n",
    "                        if pos_acc >= best_accuracy or epoch > (num_train_epoch - req_save_epoch) or epoch%force_save_per==0 or save_epoch < req_save_epoch:\n",
    "                            mlparser.save(filename=mlparser._save_model_folder+mlparser._save_model_name+str(epoch)+\"_\"+str(pos_acc))\n",
    "                            _ = mlparser.test(lang_code=test_lang_code, gold_file=gold_file, test_file=test_file, batch_size=batch_size, epoch=epoch, write_file=True)\n",
    "                            if pos_acc > best_accuracy:\n",
    "                                best_accuracy = pos_acc\n",
    "                                best_ephoc = epoch\n",
    "                                non_update_count = 0\n",
    "\n",
    "                    print(\"epoch\", epoch, \"time\",time.time()-start, \"best POS accuracy:\", best_accuracy, best_epoch)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"python tagger.py (train or predict) --test_lang\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
